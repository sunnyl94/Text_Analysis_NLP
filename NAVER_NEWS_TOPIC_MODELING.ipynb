{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 네이버 뉴스 데이터 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import urllib.request\n",
    "import requests\n",
    "\n",
    "news_data = []\n",
    "page_count = 3\n",
    "\n",
    "client_id = \"3kQhlGXxOc34pzRN58b8\"\n",
    "client_secret = \"KlTURzIdS_\"\n",
    "encText = urllib.parse.quote(\"파이썬\")\n",
    "\n",
    "for idx in range(page_count):\n",
    "    # json\n",
    "    url = \"https://openapi.naver.com/v1/search/news?query=\" + encText + \"&start=\" + str(idx * 10 + 1)\n",
    "\n",
    "    request = urllib.request.Request(url)\n",
    "    request.add_header(\"X-Naver-Client-Id\",client_id)\n",
    "    request.add_header(\"X-Naver-Client-Secret\",client_secret)\n",
    "    response = urllib.request.urlopen(request)\n",
    "    rescode = response.getcode()\n",
    "\n",
    "    if(rescode==200):\n",
    "\n",
    "        result = requests.get(response.geturl(),\n",
    "                              headers={\"X-Naver-Client-Id\":client_id,\n",
    "                                       \"X-Naver-Client-Secret\":client_secret}\n",
    "                             )\n",
    "        news_data.append(result.json())\n",
    "\n",
    "    else:\n",
    "        print(\"Error Code:\" + rescode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 네이버 뉴스만 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "naver_news_link = []\n",
    "\n",
    "for page in news_data:\n",
    "#     print(page)\n",
    "    page_news_link = []\n",
    "    \n",
    "    for item in page['items']:\n",
    "        #print(item)\n",
    "        temp_link = item['link']\n",
    "#         print(temp_link)\n",
    "        if \"naver\" in temp_link:\n",
    "            page_news_link.append(temp_link)\n",
    "    \n",
    "    naver_news_link.append(page_news_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 네이버 뉴스 기사 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from selenium import webdriver\n",
    "from tqdm import tqdm_notebook\n",
    "import requests\n",
    "import pickle\n",
    "import re\n",
    "import ast\n",
    "\n",
    "from bs4 import BeautifulSoup \n",
    "from urllib.request import urlopen\n",
    "import urllib\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome('/Users/seonil/Desktop/driver/chromedriver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seonil/opt/anaconda3/envs/test/lib/python3.6/site-packages/ipykernel_launcher.py:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3db532f9eed43d38f041ac76b08a540",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seonil/opt/anaconda3/envs/test/lib/python3.6/site-packages/ipykernel_launcher.py:10: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41e55d7d38594218b37369ad19feb743",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "964ec0f51e34403091952c4bd8a84018",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=102&oid=421&aid=0005556704\n",
      "https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=102&oid=008&aid=0004634761\n",
      "https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=101&oid=009&aid=0004841708\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a362cab1f76c499f823d8b5402237874",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=102&oid=003&aid=0010676924\n",
      "https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=102&oid=015&aid=0004594713\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "naver_news_title = []\n",
    "naver_news_content = []\n",
    "\n",
    "\n",
    "for n in tqdm_notebook(range(len(naver_news_link))):\n",
    "#     print(n)\n",
    "    news_page_title = []\n",
    "    news_page_content = []\n",
    "    \n",
    "    for idx in tqdm_notebook(range(len(naver_news_link[n]))):\n",
    "        \n",
    "        \n",
    "    # URL로 접속\n",
    "        try:\n",
    "            driver.get(naver_news_link[n][idx])\n",
    "            print(naver_news_link[n][idx])\n",
    "            \n",
    "        except:\n",
    "            print(\"Timeout!\")\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            response = driver.page_source\n",
    "            \n",
    "        except UnexpectedAlertPresentException:\n",
    "            driver.switch_to_alert().accept()\n",
    "            print(\"게시글이 삭제된 경우입니다.\")\n",
    "            continue\n",
    "        \n",
    "        soup = BeautifulSoup(response, \"html.parser\")\n",
    "        \n",
    "        # 뉴스 타이틀\n",
    "        \n",
    "        title = None\n",
    "        \n",
    "        try:\n",
    "            item = soup.find('div', class_=\"article_info\")\n",
    "            title = item.find('h3', class_=\"tts_head\").get_text()\n",
    "            #print(title)\n",
    "\n",
    "        except:\n",
    "            title = \"OUTLINK\"\n",
    "        \n",
    "#         print(title)\n",
    "        news_page_title.append(title)\n",
    "        \n",
    "        \n",
    "        # 뉴스 본문\n",
    "        \n",
    "        doc = None\n",
    "        text = \"\"\n",
    "                \n",
    "        data = soup.find_all(\"div\", {\"class\" : \"_article_body_contents\"})\n",
    "        if data:\n",
    "            for item in data:\n",
    "\n",
    "                text = text + str(item.find_all(text=True)).strip()\n",
    "                text = ast.literal_eval(text)\n",
    "                doc = ' '.join(text)\n",
    "   \n",
    "        else:\n",
    "            doc = \"OUTLINK\"\n",
    "            \n",
    "        news_page_content.append(doc.replace('\\n', ' '))\n",
    "\n",
    "                \n",
    "    naver_news_title.append(news_page_title)\n",
    "    naver_news_content.append(news_page_content)\n",
    "\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 크롤링 결과 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"naver_news_title.pk\", \"wb\") as f:\n",
    "    pickle.dump(naver_news_title, f)\n",
    "    \n",
    "with open(\"naver_news_content.pk\", \"wb\") as f:\n",
    "    pickle.dump(naver_news_content, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 토픽 모델링 (gensim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook \n",
    "from konlpy.tag import Mecab \n",
    "import numpy as np\n",
    "import string \n",
    "import re\n",
    "import warnings \n",
    "import pickle\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 전처리 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94b92cd0f8a044c3830acffd1001b2cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Preprocessing', max=5.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def read_documents(input_file_name):\n",
    "    \n",
    "    corpus = []\n",
    "    \n",
    "    with open(input_file_name, 'rb') as f:\n",
    "        temp_corpus = pickle.load(f)\n",
    "        \n",
    "    for page in temp_corpus:\n",
    "        corpus += page\n",
    "    \n",
    "    return corpus\n",
    "\n",
    "def text_cleaning(docs):\n",
    "    \n",
    "    for doc in docs:\n",
    "        doc = re.sub(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\", doc)\n",
    "\n",
    "    return docs\n",
    "\n",
    "def define_stopwords(path):\n",
    "    \n",
    "    SW = set()\n",
    "    \n",
    "    for i in string.punctuation:\n",
    "        SW.add(i)\n",
    "    \n",
    "    with open(path) as f:\n",
    "        for word in f:\n",
    "            SW.add(word)\n",
    "\n",
    "    return SW\n",
    "\n",
    "\n",
    "def text_tokenizing(corpus, tokenizer):\n",
    "    \n",
    "    mecab = Mecab()\n",
    "    token_corpus = []\n",
    "    \n",
    "\n",
    "    if tokenizer == \"noun\":\n",
    "        for n in tqdm_notebook(range(len(corpus)), desc=\"Preprocessing\"):\n",
    "            token_text = mecab.nouns(corpus[n])\n",
    "            token_text = [word for word in token_text if word not in SW and len(word) > 1]\n",
    "                \n",
    "            token_corpus.append(token_text)\n",
    "            \n",
    "    elif tokenized == \"morph\":\n",
    "        for n in tqdm_notebook(range(len(corpus)), desc=\"Preprocessing\"):\n",
    "            token_text = mecab.morphs(corpus[n])\n",
    "            token_text = [word for word in token_text if word not in SW and len(word) > 1]\n",
    "            token_corpus.append(token_text)\n",
    "\n",
    "    elif tokenizer == \"word\":\n",
    "        for n in tqdm_notebook(range(len(corpus)), desc=\"Preprocessing\"):\n",
    "            token_text = corpus[n].split()\n",
    "            token_text = [word for word in token_text if word not in SW and len(word) > 1]\n",
    "            token_corpus.append(token_text)\n",
    "        \n",
    "\n",
    "    return token_corpus\n",
    "\n",
    "\n",
    "input_file_name = \"data/naver_news_content.pk\"\n",
    "documents = read_documents(input_file_name)\n",
    "SW = define_stopwords(\"data/stopwords-ko.txt\")\n",
    "cleaned_text = text_cleaning(documents)\n",
    "tokenized_text = text_tokenizing(cleaned_text, tokenizer=\"noun\") #tokenizer= \"noun\" or \"word\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bag of words\n",
    "dictionary = corpora.Dictionary(tokenized_text)\n",
    "corpus = [dictionary.doc2bow(text) for text in tokenized_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lda model\n",
    "model = models.ldamodel.LdaModel(corpus, num_topics=4, id2word=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('교육', 0.049934156),\n",
       " ('인공지능', 0.026237886),\n",
       " ('과정', 0.020828623),\n",
       " ('한밭', 0.015528885),\n",
       " ('데이터', 0.014485034),\n",
       " ('운영', 0.014448632),\n",
       " ('세종', 0.014016202),\n",
       " ('러닝', 0.012420734),\n",
       " ('공학', 0.012376833),\n",
       " ('실습', 0.011855137)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 마지막 토픽, 10개 단어\n",
    "model.show_topic(3, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 토픽 모델링 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOPICS = 3\n",
    "\n",
    "NUM_TOPIC_WORDS = 30\n",
    "\n",
    "\n",
    "def build_doc_term_mat(documents):\n",
    "    print(\"Building document-term matrix.\")\n",
    "    dictionary = corpora.Dictionary(documents)\n",
    "    corpus = [dictionary.doc2bow(document) for document in documents]\n",
    "        \n",
    "    return corpus, dictionary\n",
    "\n",
    "\n",
    "def print_topic_words(model):\n",
    "    print(\"\\nPrinting topic words.\\n\")\n",
    "    \n",
    "    for topic_id in range(model.num_topics):\n",
    "        topic_word_probs = model.show_topic(topic_id, NUM_TOPIC_WORDS)\n",
    "        print(\"Topic ID: {}\".format(topic_id))\n",
    "        \n",
    "        for topic_word, prob in topic_word_probs:\n",
    "            print(\"\\t{}\\t{}\".format(topic_word, prob))\n",
    "            \n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building document-term matrix.\n",
      "\n",
      "Printing topic words.\n",
      "\n",
      "Topic ID: 0\n",
      "\t교육\t0.02070821076631546\n",
      "\t과정\t0.014254316687583923\n",
      "\t인공지능\t0.013551284559071064\n",
      "\t미적분\t0.011044965125620365\n",
      "\t데이터\t0.010591119527816772\n",
      "\t실습\t0.01046698447316885\n",
      "\t프로그램\t0.009771065786480904\n",
      "\t한밭\t0.009417698718607426\n",
      "\t러닝\t0.009303211234509945\n",
      "\t코딩\t0.009276061318814754\n",
      "\t플레이어\t0.00905511062592268\n",
      "\t내용\t0.00903499685227871\n",
      "\t진행\t0.00902926828712225\n",
      "\t대학\t0.008764331229031086\n",
      "\t예측\t0.008472765795886517\n",
      "\t교수\t0.008428006432950497\n",
      "\t기술\t0.008040238171815872\n",
      "\t이해\t0.00790377426892519\n",
      "\t생각\t0.007842546328902245\n",
      "\t재직\t0.007841642946004868\n",
      "\t본문\t0.007793664466589689\n",
      "\t플랫\t0.007595655508339405\n",
      "\t운영\t0.007143128663301468\n",
      "\t세종\t0.006995231844484806\n",
      "\t분석\t0.006823406089097261\n",
      "\t대학교\t0.006680227350443602\n",
      "\t신경망\t0.006381955463439226\n",
      "\t오류\t0.00629609078168869\n",
      "\t공학\t0.006060867570340633\n",
      "\t우회\t0.006028519943356514\n",
      "\n",
      "\n",
      "Topic ID: 1\n",
      "\t교육\t0.04051082953810692\n",
      "\t인공지능\t0.02221476472914219\n",
      "\t과정\t0.01519868429750204\n",
      "\t한밭\t0.012104225344955921\n",
      "\t데이터\t0.011565291322767735\n",
      "\t운영\t0.011323790065944195\n",
      "\t세종\t0.011258297599852085\n",
      "\t플레이어\t0.010682249441742897\n",
      "\t집중\t0.010288194753229618\n",
      "\t본문\t0.010107682086527348\n",
      "\t대학\t0.010045642033219337\n",
      "\t미적분\t0.00965145044028759\n",
      "\t지역\t0.009326789528131485\n",
      "\t공학\t0.009263157844543457\n",
      "\t내용\t0.009261048398911953\n",
      "\t국립대\t0.009061411954462528\n",
      "\t러닝\t0.008742022328078747\n",
      "\t시민\t0.008561631664633751\n",
      "\t기초\t0.008434399031102657\n",
      "\t프로그램\t0.008199183270335197\n",
      "\t확대\t0.007833666168153286\n",
      "\t실습\t0.007722039707005024\n",
      "\t분야\t0.007632608525454998\n",
      "\t생각\t0.007483777590095997\n",
      "\t교수\t0.00746076088398695\n",
      "\t진행\t0.007020448334515095\n",
      "\t머신\t0.006843633949756622\n",
      "\t생활\t0.006808189209550619\n",
      "\t이해\t0.006578858941793442\n",
      "\t기술\t0.006433498580008745\n",
      "\n",
      "\n",
      "Topic ID: 2\n",
      "\t교육\t0.03791317343711853\n",
      "\t인공지능\t0.019540201872587204\n",
      "\t과정\t0.01809086464345455\n",
      "\t운영\t0.012786133214831352\n",
      "\t데이터\t0.012341567315161228\n",
      "\t러닝\t0.011085571721196175\n",
      "\t본문\t0.01103175152093172\n",
      "\t내용\t0.010838870890438557\n",
      "\t공학\t0.010723600164055824\n",
      "\t세종\t0.010636040940880775\n",
      "\t한밭\t0.010431960225105286\n",
      "\t플레이어\t0.009604738093912601\n",
      "\t기술\t0.00936981663107872\n",
      "\t미적분\t0.009015314280986786\n",
      "\t프로그램\t0.00888874288648367\n",
      "\t실습\t0.00878283753991127\n",
      "\t생각\t0.00861484743654728\n",
      "\t진행\t0.008133371360599995\n",
      "\t대학\t0.008050152100622654\n",
      "\t집중\t0.0076554808765649796\n",
      "\t미분\t0.007595022674649954\n",
      "\t머신\t0.007567830849438906\n",
      "\t응용\t0.0074376570992171764\n",
      "\t코딩\t0.007178450468927622\n",
      "\t이해\t0.007029864005744457\n",
      "\t기초\t0.006930525414645672\n",
      "\t지역\t0.006738885771483183\n",
      "\t동안\t0.00649277912452817\n",
      "\t사용\t0.006459350697696209\n",
      "\t확대\t0.0062714130617678165\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corpus, dictionary = build_doc_term_mat(tokenized_text)\n",
    "\n",
    "# LDA 모델\n",
    "model = models.ldamodel.LdaModel(corpus, num_topics=NUM_TOPICS, id2word=dictionary, alpha=\"auto\", eta=\"auto\")\n",
    "print_topic_words(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
